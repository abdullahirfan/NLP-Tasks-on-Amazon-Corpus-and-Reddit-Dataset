# -*- coding: utf-8 -*-
"""Assingment4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fVMtNIH-nTeUQcayh5MuQLCxBGi4NG2I
"""

import numpy as np
import scipy as sc
import gensim as g
import pandas as pd
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
import re
import csv

from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split

from keras.models import Sequential
from keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional
from keras.preprocessing.text import text_to_word_sequence, Tokenizer
from keras.preprocessing.sequence import pad_sequences

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
# %config IPCompleter.greedy=True
from keras.callbacks import EarlyStopping
from keras.regularizers import l2
from keras import layers
from keras.layers import Flatten

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

"""# **DATA PREPROCESSING: LOADING**"""

files=['train_pos.csv','train_neg.csv','val_pos.csv','val_neg.csv',
       'test_pos.csv','test_neg.csv']

def load_data(file):
    def retreive_from_csv(words):
        ls = [i for i in words]
        return ls
    
    f = open(file)
    lists=[]
 
    for j in f:
        lists.append((retreive_from_csv(eval(j))))
     
    return lists

train_pos_list = load_data(files[0])
train_neg_list=load_data(files[1])

val_pos_list = load_data(files[2])
val_neg_list=load_data(files[3])

test_pos_list=load_data(files[4])
test_neg_list=load_data(files[5])

"""# DATA PREPROCESSING: GETTING WORD EMBEDDINGS

Word 2 Vec is only on training set and NOT on the entire corpus
"""

from gensim.models import Word2Vec

w2v_saved_model = g.models.KeyedVectors.load_word2vec_format("word2vec.bin",binary=True)

print('Number of words in this pre-trained w2v model:', len(w2v_saved_model.wv.vocab))
print('Dimension of w2v:', w2v_saved_model.wv.vector_size)

"""# DATA PREPROCESSING: ASSIGNING OUTCOME TO CORPUS AS POSITIVE/NEGATIVE"""

positive_train=pd.DataFrame(columns=['sentence', 'outcome'])
positive_train['sentence'] = train_pos_list
positive_train['outcome']  = [np.array([1,0])]*len(train_pos_list) 

positive_test=pd.DataFrame(columns=['sentence', 'outcome'])
positive_test['sentence'] = test_pos_list
positive_test['outcome']= [np.array([1,0])]*len(test_pos_list)  

positive_val=pd.DataFrame(columns=['sentence', 'outcome'])
positive_val['sentence'] = val_pos_list
positive_val['outcome']= [np.array([1,0])]*len(val_pos_list)    

negitive_train=pd.DataFrame(columns=['sentence', 'outcome'])
negitive_train['sentence'] = train_neg_list
negitive_train['outcome']  = [np.array([0,1])]*len(train_neg_list) 

negitive_test=pd.DataFrame(columns=['sentence', 'outcome'])
negitive_test['sentence'] = test_neg_list
negitive_test['outcome'] = [np.array([0,1])]*len(test_neg_list)  

negitive_val=pd.DataFrame(columns=['sentence', 'outcome'])
negitive_val['sentence'] = val_neg_list
negitive_val['outcome']= [np.array([0,1])]*len(val_neg_list)

train_df=positive_train.append(negitive_train)
test_df=positive_test.append(negitive_test)
val_df=positive_val.append(negitive_val)

train_df=train_df.sample(frac=1,random_state=10)
train_df.reset_index(inplace=True,drop=True)

test_df=test_df.sample(frac=1,random_state=10)
test_df.reset_index(inplace=True,drop=True)


val_df=val_df.sample(frac=1,random_state=10)
val_df.reset_index(inplace=True,drop=True)

test_df.head()

"""# TOKENIZATION OF CORPUS: CONVERTING INTO NUMBERS & PADDING"""

tokenizer = Tokenizer(num_words=len(w2v_saved_model.wv.vocab))
  tokenizer.fit_on_texts(np.array(train_df["sentence"]))
  seq = tokenizer.texts_to_sequences([" ".join(sent) for sent in train_df["sentence"]])
  #print('90th Percentile Sentence Length:', np.percentile([len(s) for s in seq], 99))
  X_train = pad_sequences(seq, maxlen=30, padding='post', truncating='post')
  y_train = train_df['outcome']

def tokenizing_corpus(lists,token):
  # Convert the sequence of words to sequnce of indices
  seq = token.texts_to_sequences([" ".join(sent) for sent in lists["sentence"]])
  #print('90th Percentile Sentence Length:', np.percentile([len(s) for s in seq], 99))
  X = pad_sequences(seq, maxlen=30, padding='post', truncating='post')
  y = lists['outcome']
  return X,y

X_test,y_test=tokenizing_corpus(test_df,(tokenizer))
X_val,y_val=tokenizing_corpus(val_df,(tokenizer))

y_train = np.array([i for i in y_train])
y_test = np.array([i for i in y_test])
y_val = np.array([i for i in y_val])

y_train.shape

"""# PREPARING THE WEIGHTS"""

embeddings_matrix = np.random.uniform(-0.05, 0.05, size=(len(tokenizer.word_index)+1, 300)) # +1 is because the matrix indices start with 0

for word, i in tokenizer.word_index.items(): # i=0 is the embedding for the zero padding
    try:
        embeddings_vector = w2v_saved_model[word]
    except KeyError:
        embeddings_vector = None
    if embeddings_vector is not None:
        embeddings_matrix[i] = embeddings_vector

embeddings_matrix.shape

"""# BASELINE MODEL DEVELOPMENT"""

model = Sequential()
model.add(Embedding(input_dim=(len(tokenizer.word_index)+1),output_dim=300, input_length=30, weights = [embeddings_matrix], trainable=False, name='word_embedding_layer', mask_zero=False))


model.add(Flatten())

model.add(Dropout(rate=0.2, name='dropout_layer'))

model.add(layers.Dense(32,  name='hidden_layer',activation = "relu", kernel_regularizer=l2(0.01)))

model.add(layers.Dense(2, activation='softmax', name='output_layer'))

model.summary()

model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])

"""# TRAINING THE BASELINE MODEL"""

model.fit(X_train, y_train, batch_size=256, epochs=5,validation_data=(X_val, y_val))

"""# BASELINE MODEL ACCURACY"""

score, acc = model.evaluate(X_test, y_test, batch_size=32)


print("Accuracy on Test Set = {0:4.3f}".format(acc))

"""# PERFORMANCE TUNING ON VALIDATION SET: TUNING ACTIVATIONS"""

activations = ["sigmoid", "tanh"]
results=[]
concat_result=[]

for i in range(len(activations)):
        
        # create model
        model = Sequential()
        model.add(Embedding(input_dim=(len(tokenizer.word_index)+1),output_dim=300,input_length=30,weights = [embeddings_matrix], trainable=False, name='word_embedding_layer',mask_zero=False))


        model.add(Flatten())

       

        model.add(layers.Dense(32,  name='hidden_layer',activation = activations[i], kernel_regularizer=l2(0.01)))
        
        model.add(Dropout(rate=0.2, name='dropout_layer'))

        model.add(layers.Dense(2, activation='softmax', name='output_layer'))

        model.summary()

        
        
        # Compile model
        model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])

        model.fit(X_train, y_train, batch_size=256, epochs=5,validation_data=(X_val, y_val))

        score, acc = model.evaluate(X_test, y_test, batch_size=32)
        print("Accuracy on Test Set = {0:4.3f}".format(acc))
        
        # Create a column from the list
        
        
        results.append(acc) 
        
   
        concat_result=pd.DataFrame(results)
        results=[]
    
        concat_result

"""# PERFORMANCE TUNING ON VALIDATION SET: DIFFERENT DROPOUT RATES"""

dropouts= [0.2,0.25, 0.3, 0.4,0.5]
results_dropout=[]
concat_result_dropout=[]


for i in range(len(dropouts)):
        
        # create model
        model = Sequential()
        model.add(Embedding(input_dim=(len(tokenizer.word_index)+1),output_dim=300,input_length=30,weights = [embeddings_matrix], trainable=False, name='word_embedding_layer',mask_zero=False))


        model.add(Flatten())

        model.add(layers.Dense(32,  name='hidden_layer',activation = 'tanh', kernel_regularizer=l2(0.01)))
        
        model.add(Dropout(rate=dropouts[i], name='dropout_layer'))

        model.add(layers.Dense(2, activation='softmax', name='output_layer'))

        model.summary()

        
        
        # Compile model
        model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])

        model.fit(X_train, y_train, batch_size=256, epochs=5,validation_data=(X_val, y_val))

        score, acc = model.evaluate(X_test, y_test, batch_size=32)
        print("Accuracy on Test Set = {0:4.3f}".format(acc))
        
        # Create a column from the list
        
        
        results.append(acc) 
        
   
        concat_result=pd.DataFrame(results)
        results=[]
    
        concat_result