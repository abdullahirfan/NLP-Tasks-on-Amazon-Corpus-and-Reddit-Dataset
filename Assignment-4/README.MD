Report your classification accuracy results in a table
with three different activation functions in the hidden layer(ReLU, sigmoid and tanh).What effect do activation functions have on your results?
What effect does addition of L2-norm regularization have on the results?
What effect doesdropout have on the results?
Explain your intuitionsbriefly
